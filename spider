import os
import argparse
import requests
import shutil
from urllib.parse import urlparse
from bs4 import BeautifulSoup

"""
Spider Gonzalez - Download images from a web page recursively.

Usage:
spider.py [-r] [-l <levels>] [-p <path>] <url>

Options:
-r, --recursive Download images from pages linked to the URL recursively.
-l, --level <levels> Maximum level of recursion when downloading images [default: 1].
-p, --path <path> Path to the folder where images will be saved [default: ./images].
-c --clean Delete the image folder before scraping.
<url> URL of the web page to analyze.

Examples:
spider.py -r -l 2 -p ./images https://www.example.com
spider.py -r -p ./images https://www.example.com
spider.py https://www.example.com

"""

def get_links(content):
    soup = BeautifulSoup(content, "html.parser")
    links = []
    for img in soup.find_all("img"):
        src = img.get("src")
        if src:
            links.append(src)
    return links


def download_images(url, folder, level, downloaded_files):
    page = requests.get(url)
    soup = BeautifulSoup(page.content, "html.parser")
    for img in soup.find_all("img"):
        src = img.get("src")
        if src and not src.startswith("data:"):
            filename = os.path.join(folder, os.path.basename(src))
            if filename in downloaded_files:
                #print(f"\033[1;33mSkipping {filename} (already downloaded)\033[0m")
                continue
            print(f"\033[1;32mDownloading {filename}\033[0m \033[1;33mfrom {src}\033[0m in {level}")
            downloaded_files.add(filename)
            try:
                with open(filename, "wb") as f:
                    f.write(requests.get(src).content)
            except:
                print(f"Failed to download {src}")

    if level > 1:
        for link in soup.find_all("a"):
            href = link.get("href")
            if href and href.startswith("{args.url}"):
                download_images(href, folder, level - 1, downloaded_files)


def scrape_images(path_or_url, folder, recursive=False, level=1, downloaded_files=set()):
    # Check if the path_or_url is a valid URL
    try:
        url = urlparse(path_or_url)
        if url.scheme and url.netloc:
            if level < 1:
                return
            if not os.path.exists(folder):
                os.makedirs(folder)
            # If the path_or_url is a URL, execute the current logic
            download_images(path_or_url, folder, level, downloaded_files)
            if recursive:
                page = requests.get(path_or_url)
                soup = BeautifulSoup(page.content, "html.parser")
                for link in soup.find_all("a"):
                    href = link.get("href")
                    if href and href.startswith("http"):
                        parsed_href = urlparse(href)
                        parsed_url = urlparse(path_or_url)
                        if parsed_href.netloc == parsed_url.netloc:
                            scrape_images(href, folder, recursive, level - 1, downloaded_files)
            return
    except ValueError:
        # If the path_or_url is not a valid URL, it must be a local path
        pass

    # Search for images in a local directory
    for root, _, files in os.walk(path_or_url):
        for file in files:
            if file.endswith(".jpg") or file.endswith(".jpeg") or file.endswith(".png") or file.endswith(".gif"):
                filename = os.path.join(root, file)
                if filename in downloaded_files:
                    print(f"Skipping {filename} (already downloaded)")
                    continue
                print(f"Downloading {filename}")
                downloaded_files.add(filename)
                try:
                    shutil.copy(filename, folder)
                except:
                    print(f"Failed to copy {filename} to {folder}")

    # If recursive is true, search for images in subdirectories
    if recursive:
        for root, dirs, _ in os.walk(path_or_url):
            for dir in dirs:
                new_path = os.path.join(root, dir)
                scrape_images(new_path, folder, recursive, level - 1, downloaded_files)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download images from a web page.")
    parser.add_argument("url", type=str, nargs="?", help="URL of the web page")
    parser.add_argument("-r", "--recursive", action="store_true", help="Download images from pages linked to the URL recursively")
    parser.add_argument("-l", "--level", type=int, default=1, help="Maximum level of recursion when downloading images (default: 1)")
    parser.add_argument("-p", "--path", type=str, default="images.url", help="Path to the folder where images will be saved (default: images.url)")
    parser.add_argument('-c', '--clean', action='store_true',  help='Delete the image folder before scraping.')

    args = parser.parse_args()

    if args.clean:
        shutil.rmtree(args.path)
        print("Directory images have been deleted.")
        exit

    if args.url is None:
        recursive = input("Download images from pages linked to the URL recursively? (y/n) ")
        if recursive.lower() == "y":
            args.recursive = True
        else:
            args.recursive = False

        level = int(input("Maximum level of recursion when downloading images (default: 1): ") or "1")
        args.level = level

        url = input("URL of the web page: ")
        args.url = url

        folder = input("Path to the folder where images will be saved (default: images.url): ") or "images.url"
        args.path = folder

    scrape_images(args.url, args.path, args.recursive, args.level)
